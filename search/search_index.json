{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Tariq Khasawneh \u00b6 Hello World! \u00b6 Welcome on board! This folder contains everything related to my personal blog.","title":"Tariq Khasawneh"},{"location":"#tariq-khasawneh","text":"","title":"Tariq Khasawneh"},{"location":"#hello-world","text":"Welcome on board! This folder contains everything related to my personal blog.","title":"Hello World!"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/0-Data%20Structure%20and%20Algorithms%20Objectives/","text":"Data Structure & Algorithms Series \u00b6 created on: 2022-07-31 16:17 Objectives \u00b6 The series structure is inspired by Jay Wengrow's book - A Common-Sense guide to Data Structure and Algorithms. Which is outlined as follows: Chapter 1: Why DS are important? Chapter 2: Why Algorithms are important? Chapter 3, 4, 5: Big O and using it as a metric to optimize algorithms worst case runtime Bubble sort Selection sort Chapter 6: Beyond optimizing for the worst case Insertion sort Chapter 7: Applications of Big O Chapter 8: Hash Tables Chapter 9: Stacks and Queues Chapter 10, 11, 13: Recursion Chapter 11: Node-based data structures Chapter 12: Dynamic Programming Chapter 14: Linked Lists Chapter 15 Trees Chapter 16: Heaps Chapter 17: Tries Chapter 18: Graph Algorithms Chapter 19: Dealing with Space Constraints Chapter 20: Techniques for Code Optimization To cover all the topics discussed in the books, we'll cover its table of content into chunks, each chunk will be presented in a series of videos. The first series would cover the following components: - Big O notation - What is it used for and why do we use it? - It's usefulness as a tool and using it to optimize algorithms - We'll visualize big O on basic chunks of code (for loops, conditionals, etc.) - Recursion - An introduction to recursion. - Why is it useful and why do we use it? - Back to Big O in the context of recursion. - Sorting Algorithms - Applications of sorting algorithms and why do we need them. - Go over the algorithms: bubble sort, insertion sort, quick sort, merge sort, selection sort - Implementation in code . - Analyzing their performance using Big O. - Project - The final project would be a live stream where we create a web app that enables the user to select a sorting algorithm and visualize it online, similar to this project . Project Description Visualizng Searching Algorithms In this project we will create and host a web app that enables the user to visualize the most popular searching algorithms, namely the bubble sort, insertion sort, merge sort, and quick sort Estimated delivery time: \u00b6 Component Delivery Time Total Duration of Component Videos Number of videos Big O Notation 1 week 2 hours around 10 videos Recursion 1 week 2 hours around 5 videos Sorting Algorithms 2 weeks 10 hours around 10 videos Project 1 week 4 hours single video stream Resources: A Common-Sense guide to Data Structure and Algorithms Leet code Cracking the coding interview index","title":"Data Structure & Algorithms Series"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/0-Data%20Structure%20and%20Algorithms%20Objectives/#data-structure-algorithms-series","text":"created on: 2022-07-31 16:17","title":"Data Structure &amp; Algorithms Series"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/0-Data%20Structure%20and%20Algorithms%20Objectives/#objectives","text":"The series structure is inspired by Jay Wengrow's book - A Common-Sense guide to Data Structure and Algorithms. Which is outlined as follows: Chapter 1: Why DS are important? Chapter 2: Why Algorithms are important? Chapter 3, 4, 5: Big O and using it as a metric to optimize algorithms worst case runtime Bubble sort Selection sort Chapter 6: Beyond optimizing for the worst case Insertion sort Chapter 7: Applications of Big O Chapter 8: Hash Tables Chapter 9: Stacks and Queues Chapter 10, 11, 13: Recursion Chapter 11: Node-based data structures Chapter 12: Dynamic Programming Chapter 14: Linked Lists Chapter 15 Trees Chapter 16: Heaps Chapter 17: Tries Chapter 18: Graph Algorithms Chapter 19: Dealing with Space Constraints Chapter 20: Techniques for Code Optimization To cover all the topics discussed in the books, we'll cover its table of content into chunks, each chunk will be presented in a series of videos. The first series would cover the following components: - Big O notation - What is it used for and why do we use it? - It's usefulness as a tool and using it to optimize algorithms - We'll visualize big O on basic chunks of code (for loops, conditionals, etc.) - Recursion - An introduction to recursion. - Why is it useful and why do we use it? - Back to Big O in the context of recursion. - Sorting Algorithms - Applications of sorting algorithms and why do we need them. - Go over the algorithms: bubble sort, insertion sort, quick sort, merge sort, selection sort - Implementation in code . - Analyzing their performance using Big O. - Project - The final project would be a live stream where we create a web app that enables the user to select a sorting algorithm and visualize it online, similar to this project . Project Description Visualizng Searching Algorithms In this project we will create and host a web app that enables the user to visualize the most popular searching algorithms, namely the bubble sort, insertion sort, merge sort, and quick sort","title":"Objectives"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/0-Data%20Structure%20and%20Algorithms%20Objectives/#estimated-delivery-time","text":"Component Delivery Time Total Duration of Component Videos Number of videos Big O Notation 1 week 2 hours around 10 videos Recursion 1 week 2 hours around 5 videos Sorting Algorithms 2 weeks 10 hours around 10 videos Project 1 week 4 hours single video stream Resources: A Common-Sense guide to Data Structure and Algorithms Leet code Cracking the coding interview index","title":"Estimated delivery time:"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/1-Why%20Data%20Structures%3F/","text":"Before delving into Big O notation, a technique used to analyze the efficiency of different algorithms, we need to study the building blocks that the algorithms we are analyzing operate on. Namely, Data Structures. Why Data Structures Matter? \u00b6 Data structures help us increase the quality of our code. How do we compare the quality of two snippets of code that achieve the same functionality? Do we say, that since both pieces of code achieve the same objective, they are therefore equivalent? This is clearly not the case, blocks of code differ in quality. The quality of the code depends on two main factors: 1. Maintainability: for the code to be easily maintainable, it should be easily understood by people who haven't written it. Therefore, it has to be organized, readable, and modular. 2. Efficiency: An efficient piece of code is one that runs faster, but also one that doesn't consume too many resources. i.e. it's code that attempts to strike a balance between its space and time requirements. To demonstrate how two pieces of code achieving the same functionality might differ in efficiency, let's create a function that simply prints the arguments passed to it. const printArgsV1(initial, final){ for(let i = initial; i<=final; i++){ if((i==initial)||(i==final)){ console.log(i); } } } const printArgsV2(initial, final){ console.log(initial); console.log(final); } If we compare the two versions of our function, clearly the second version is much more efficient, in the first version we are looping over all the numbers between initial and final, if initial = 0 and final = 1,000,000, we are essentially doing one million iteration, just to print the values of initial and final. In the second version, on the other hand, we print the two values without a single iteration. The first version is an inefficient piece of code, and a naive mistake that no programmer would typically make. However, there are more subtle and nuanced optimizations that even seasoned programmers tend to miss. The study of data structures and algorithms would help us spot such mistakes, and avoid them in the first place. What are Data Structures? \u00b6 Data structures are ways of organizing pieces of data. let a = 1; let b = 2; let c = 3; console.log(a+b+c); //outputs 6 const arr = [1,2,3] console.log(arr[0]+arr[1]+arr[2]); //outputs 6 In both cases, the result is the same. Which is the sum of the numbers 1, 2, and 3. However, when we execute the console.log() statement, the source of the data to be summed is different. In the first snippet we are retrieving the values 1, 2, 3 from memory, where each value is assigned to a separate variable a, b, c. In the second case, the values are organized in an array, all of them are stored in a single data structure, called an array. Instead of being stored separately, each in its unique variable. When building a piece of software, a web application for instance. How we choose to organize our data is of great significance. Because the way data is organized affects the storage and the retrieval of that data. Therefore, directly affecting the performance. If we take tiktok's feed for instance, those videos you are scrolling through have to be stored somewhere, how they are stored, displayed and organized would directly affect the experience of the millions of users who are requesting the data simultaneously while scrolling through the feed. To summarize: Data structures are ways of organizing data in memory, knowing what data structure to choose for a given problem would directly affect the efficiency of manipulating the organized data, i.e. adding to it, searching it, retrieving from it, or removing it. Data Structure Operations \u00b6 Read: Answering the question, what value resides in a given location of the data structure? Search: Answering the question, does the value exist in the data structure, if it does, what's the location of that value? Insert: Adding a given value to a specific location in the data structure. Delete Removing a particular value from the data structure, or removing the value in a given location. Choosing a Data Structure \u00b6 Our choice of a data structure is critical, because the four main operations carried out on a data structure (Read, Search, Insert, Delete), the efficiency of these operations is affected by the data structure we are operating on. Speed as a performance metric \u00b6 \"If you take away just one thing from this book, let it be this: when we measure how \u201cfast\u201d an operation takes, we do not refer to how fast the operation takes in terms of pure time , but instead in how many steps it takes.\" Jay Wngrow- A common sense guide to data structure and algorithms We've seen a demonstration of this in [[Why Data Structures Matter]], where printArgs2 was faster because it took a fraction of the number of steps (iterations) taken in printArgsV1 . Why do we measure speed of an operation ([[Time Complexity]]) by the number of steps instead of the time it takes? Because, the time an operation takes is a function of the hardware the operation is running on. While, the number of steps of an operation is only dependent on the implementation of the operation, i.e. the piece of code which is what we're interested in. The Array Data Structure \u00b6 The first data structure we are going to discuss, is the simplest and the wildly used, the arrays. Arrays are contiguous space in memory, where the computer associates the address of the zeroth element of the array and its with the array's address. ![[Pasted image 20220801185022.png]] As we've mentioned earlier, the importance of the choice of the data structure lies in the fact that it affects the efficiency of the four main operations. Before going through each of these operations with respect to the array data structure, here is a list of the operations and their efficiency. The cost of each operation on the array: Operation Best Case Worst Case [[Read-Arrays]] 1 step 1 step [[Search-Arrays]] 1 step N steps [[Insert-Arrays]] 1 step N+1 steps [[Delete-Arrays]] 1 step N steps The Array Operations \u00b6 Arrays Read Operation \u00b6 created on: 2022-08-01 14:28 As we've mentioned in [[Speed as a Performance Measure]], we measure the speed of an operation by the number of steps it takes. To read a variable from memory, it takes a single step, which is a retrieval from memory step. If we take the [[array]] data structure. To read an element from an array, it also takes a single step, that's quite fast. And is attributed to the way the data is organized in an array. To understand this, let us see what happens when we define an array: let arr = ['a','b','c','d']; when the array arr is defined, the computer has to store the data in memory, an array by definition in a contiguous space in memory, meaning, the values 'a', 'b', 'c', 'd' are going to be stored in sequential addresses in memory. for example: value 'a' at memory location 005 value 'b' at memory location 006 value 'c' at memory location 007 value 'd' at memory location 008 let x = arr[0] let y = arr[3] Each array stored in memory is associated with an address, the address of the element in the zeroth index, in our example, array arr would be associated with the address 005. Accessing any element other than the one stored at the zeroth element would be equivalent to accessing the element at the address: array's address + index we are trying to access. Consequently, the first line is equivalent to: go to the address 005 of the array arr , add 0 to the address (The index we're reading), read the value at the address 005 + 0, then assign it to the variable x. That's two steps, a read step, and an assignment step. Similarly, the second line is equivalent to: go to the address 005, add 3 to it (the index we're reading), read the value at the address 005+3, then assign it to the variable x. Also, two steps, a read step and an assignment step. Note that reading from an array costs exactly one step regardless of the location (index) we are interested in reading from. This is why arrays as a data structure are considered attractive, it's because it costs one step to read from any position in the array. Arrays Search \u00b6 Based on the definition of the search operation in [[Data Strcture Operations]], if we are looking for a particular value in an array, it means that we don't have the index of the value we are looking for, if we don't have the index, it means that we can't read it. Therefore, we need to look for the index where the value of interest resides. Assume there is a defined function search , that takes two arguments, an array and a value to look for within the array, and returns the index in which the value was found. arr = ['a','b','c','d']; locA = search(arr,'a'); locC = search(arr,'c'); Executing the function search(arr,'a') based on the previous definition of the function would return the value 0, and search(arr,'c') would return the value 2. To find the index where the value is stored, the computer looks for the value started at the address of the array (which is at the zeroth index), if the value is found there it returns the index, otherwise it proceeds to the adjacent address (address + 1), until it traverses the full array. Therefore, if it finds the value at index 0, it takes it one step, if it finds it at index 3 it takes 4 steps, if the array is of size 1000 and the element is at index 999, it takes it 999 steps to find it. And if the array is of size n, and the value is at the last index of the array, then it takes the search n steps to find the value. This is known as linear search, it's one search algorithm, there are many others that are more efficient as we'll see later. Arrays Insert \u00b6 Inserting data into an array is dependent on the position we are inserting the data in. Appending the data to the array takes one step. ```js let arr = ['a','b','c']; arr.push('d'); pushing the value d to the array `arr` , is equivalent to telling the computer to go to the address of `arr` (which is the address of the zeroth element in the array), adding the size of the array to it, reaching the address where d is supposed to be added, i.e. at the end of the array. When an array is stored in memory, the computer keeps track of the array's size. [[The Array Data Structure]] - Inserting in a position other than the end of the array. If we want to insert the value \"x\" at index 2 of the array `['a','b','c']`, then we would need to shift c to index 2 for x to take its place, similarly if we want to insert x at index 1, we would need to shift both c and b to the right for x to be inserted. We notice that inserting at the first index of the array would result in the largest number of steps required for insertion. Namely, N + 1 steps, N shifting steps and 1 insertion step, where N is the size of the array. ### Arrays Delete Deleting is very similar to the [[Insert-Arrays]] operation. - Deleting the final element takes exactly one step. Assume we have a function delete, that removes the value stored at the passed index. ```js let arr = ['a','b','c']; arr.delete(2); the array would become ['a','b'] , however if we want to delete element b: let arr = ['a','b','c']; arr.delete(1); Thiss would result in removing the value b, then moving the value c from index 2 to index 1 to replace the location of the index that was occupied by b, this done to preserve the array as a data structure ofcontiguouss locations.Similarlyy if we want to delete the value at index 0, it would cost us a step for deletion, and two steps to shift b to index 0, and c to index 1. Consequently,in ann array of size N, deleting its zerothh element would cost us N steps. The Set Data Structure \u00b6 There are differentt types of sets, array-based sets in particular are identical to arrays with one additional constraints. The constraint being that it doesn't allow storing duplicates. This would have an implication on one of the operations, let's investigate the speed of each of the operations: 1. Read: it takes one step to read a value at a given index, just like an array. 2. Search: it takes the 1 step in the best case and N steps in the worst case to find an element in a set, just like arrays 3. Insertion: To insert an element into the set, we have to make sure that the element doesn't already exist in the set. Therefore, an insertion operation consists of search (to look for that element), and insertion steps to insert it. i.e in the best case, it would take N+1 steps to insert an element at the end of the array (N steps for search and 1 for insertion), and in the worst case, which is inserting an element at the beginning of the array it would require N steps of search. And N steps of shifting the elements to the right to free up space at the zeroth index and 1 step to insert the new value. i.e. a total of 2N+1 steps. 4. Deletion: it takes the same number of steps as arrays, one step for deletion and the number of elements to the right of the deleted element for shifting. If we're deleting the last element of the set, then we need only one step (to delete), if we are deleting the first element of an array of size N, then we need 1 step to delete the element and N-1 steps to shift the elements to the left, which is a total of N steps. To sum up: The cost of each operation in an array-based set: Operation Best Case Worst Case Read 1 step 1 step Search 1 step N steps Insert N+1 2N+1 steps Delete 1 step N steps","title":"1 Why Data Structures?"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/1-Why%20Data%20Structures%3F/#why-data-structures-matter","text":"Data structures help us increase the quality of our code. How do we compare the quality of two snippets of code that achieve the same functionality? Do we say, that since both pieces of code achieve the same objective, they are therefore equivalent? This is clearly not the case, blocks of code differ in quality. The quality of the code depends on two main factors: 1. Maintainability: for the code to be easily maintainable, it should be easily understood by people who haven't written it. Therefore, it has to be organized, readable, and modular. 2. Efficiency: An efficient piece of code is one that runs faster, but also one that doesn't consume too many resources. i.e. it's code that attempts to strike a balance between its space and time requirements. To demonstrate how two pieces of code achieving the same functionality might differ in efficiency, let's create a function that simply prints the arguments passed to it. const printArgsV1(initial, final){ for(let i = initial; i<=final; i++){ if((i==initial)||(i==final)){ console.log(i); } } } const printArgsV2(initial, final){ console.log(initial); console.log(final); } If we compare the two versions of our function, clearly the second version is much more efficient, in the first version we are looping over all the numbers between initial and final, if initial = 0 and final = 1,000,000, we are essentially doing one million iteration, just to print the values of initial and final. In the second version, on the other hand, we print the two values without a single iteration. The first version is an inefficient piece of code, and a naive mistake that no programmer would typically make. However, there are more subtle and nuanced optimizations that even seasoned programmers tend to miss. The study of data structures and algorithms would help us spot such mistakes, and avoid them in the first place.","title":"Why Data Structures Matter?"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/1-Why%20Data%20Structures%3F/#what-are-data-structures","text":"Data structures are ways of organizing pieces of data. let a = 1; let b = 2; let c = 3; console.log(a+b+c); //outputs 6 const arr = [1,2,3] console.log(arr[0]+arr[1]+arr[2]); //outputs 6 In both cases, the result is the same. Which is the sum of the numbers 1, 2, and 3. However, when we execute the console.log() statement, the source of the data to be summed is different. In the first snippet we are retrieving the values 1, 2, 3 from memory, where each value is assigned to a separate variable a, b, c. In the second case, the values are organized in an array, all of them are stored in a single data structure, called an array. Instead of being stored separately, each in its unique variable. When building a piece of software, a web application for instance. How we choose to organize our data is of great significance. Because the way data is organized affects the storage and the retrieval of that data. Therefore, directly affecting the performance. If we take tiktok's feed for instance, those videos you are scrolling through have to be stored somewhere, how they are stored, displayed and organized would directly affect the experience of the millions of users who are requesting the data simultaneously while scrolling through the feed. To summarize: Data structures are ways of organizing data in memory, knowing what data structure to choose for a given problem would directly affect the efficiency of manipulating the organized data, i.e. adding to it, searching it, retrieving from it, or removing it.","title":"What are Data Structures?"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/1-Why%20Data%20Structures%3F/#data-structure-operations","text":"Read: Answering the question, what value resides in a given location of the data structure? Search: Answering the question, does the value exist in the data structure, if it does, what's the location of that value? Insert: Adding a given value to a specific location in the data structure. Delete Removing a particular value from the data structure, or removing the value in a given location.","title":"Data Structure Operations"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/1-Why%20Data%20Structures%3F/#choosing-a-data-structure","text":"Our choice of a data structure is critical, because the four main operations carried out on a data structure (Read, Search, Insert, Delete), the efficiency of these operations is affected by the data structure we are operating on.","title":"Choosing a Data Structure"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/1-Why%20Data%20Structures%3F/#speed-as-a-performance-metric","text":"\"If you take away just one thing from this book, let it be this: when we measure how \u201cfast\u201d an operation takes, we do not refer to how fast the operation takes in terms of pure time , but instead in how many steps it takes.\" Jay Wngrow- A common sense guide to data structure and algorithms We've seen a demonstration of this in [[Why Data Structures Matter]], where printArgs2 was faster because it took a fraction of the number of steps (iterations) taken in printArgsV1 . Why do we measure speed of an operation ([[Time Complexity]]) by the number of steps instead of the time it takes? Because, the time an operation takes is a function of the hardware the operation is running on. While, the number of steps of an operation is only dependent on the implementation of the operation, i.e. the piece of code which is what we're interested in.","title":"Speed as a performance metric"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/1-Why%20Data%20Structures%3F/#the-array-data-structure","text":"The first data structure we are going to discuss, is the simplest and the wildly used, the arrays. Arrays are contiguous space in memory, where the computer associates the address of the zeroth element of the array and its with the array's address. ![[Pasted image 20220801185022.png]] As we've mentioned earlier, the importance of the choice of the data structure lies in the fact that it affects the efficiency of the four main operations. Before going through each of these operations with respect to the array data structure, here is a list of the operations and their efficiency. The cost of each operation on the array: Operation Best Case Worst Case [[Read-Arrays]] 1 step 1 step [[Search-Arrays]] 1 step N steps [[Insert-Arrays]] 1 step N+1 steps [[Delete-Arrays]] 1 step N steps","title":"The Array Data Structure"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/1-Why%20Data%20Structures%3F/#the-array-operations","text":"","title":"The Array Operations"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/1-Why%20Data%20Structures%3F/#arrays-read-operation","text":"created on: 2022-08-01 14:28 As we've mentioned in [[Speed as a Performance Measure]], we measure the speed of an operation by the number of steps it takes. To read a variable from memory, it takes a single step, which is a retrieval from memory step. If we take the [[array]] data structure. To read an element from an array, it also takes a single step, that's quite fast. And is attributed to the way the data is organized in an array. To understand this, let us see what happens when we define an array: let arr = ['a','b','c','d']; when the array arr is defined, the computer has to store the data in memory, an array by definition in a contiguous space in memory, meaning, the values 'a', 'b', 'c', 'd' are going to be stored in sequential addresses in memory. for example: value 'a' at memory location 005 value 'b' at memory location 006 value 'c' at memory location 007 value 'd' at memory location 008 let x = arr[0] let y = arr[3] Each array stored in memory is associated with an address, the address of the element in the zeroth index, in our example, array arr would be associated with the address 005. Accessing any element other than the one stored at the zeroth element would be equivalent to accessing the element at the address: array's address + index we are trying to access. Consequently, the first line is equivalent to: go to the address 005 of the array arr , add 0 to the address (The index we're reading), read the value at the address 005 + 0, then assign it to the variable x. That's two steps, a read step, and an assignment step. Similarly, the second line is equivalent to: go to the address 005, add 3 to it (the index we're reading), read the value at the address 005+3, then assign it to the variable x. Also, two steps, a read step and an assignment step. Note that reading from an array costs exactly one step regardless of the location (index) we are interested in reading from. This is why arrays as a data structure are considered attractive, it's because it costs one step to read from any position in the array.","title":"Arrays Read Operation"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/1-Why%20Data%20Structures%3F/#arrays-search","text":"Based on the definition of the search operation in [[Data Strcture Operations]], if we are looking for a particular value in an array, it means that we don't have the index of the value we are looking for, if we don't have the index, it means that we can't read it. Therefore, we need to look for the index where the value of interest resides. Assume there is a defined function search , that takes two arguments, an array and a value to look for within the array, and returns the index in which the value was found. arr = ['a','b','c','d']; locA = search(arr,'a'); locC = search(arr,'c'); Executing the function search(arr,'a') based on the previous definition of the function would return the value 0, and search(arr,'c') would return the value 2. To find the index where the value is stored, the computer looks for the value started at the address of the array (which is at the zeroth index), if the value is found there it returns the index, otherwise it proceeds to the adjacent address (address + 1), until it traverses the full array. Therefore, if it finds the value at index 0, it takes it one step, if it finds it at index 3 it takes 4 steps, if the array is of size 1000 and the element is at index 999, it takes it 999 steps to find it. And if the array is of size n, and the value is at the last index of the array, then it takes the search n steps to find the value. This is known as linear search, it's one search algorithm, there are many others that are more efficient as we'll see later.","title":"Arrays Search"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/1-Why%20Data%20Structures%3F/#arrays-insert","text":"Inserting data into an array is dependent on the position we are inserting the data in. Appending the data to the array takes one step. ```js let arr = ['a','b','c']; arr.push('d'); pushing the value d to the array `arr` , is equivalent to telling the computer to go to the address of `arr` (which is the address of the zeroth element in the array), adding the size of the array to it, reaching the address where d is supposed to be added, i.e. at the end of the array. When an array is stored in memory, the computer keeps track of the array's size. [[The Array Data Structure]] - Inserting in a position other than the end of the array. If we want to insert the value \"x\" at index 2 of the array `['a','b','c']`, then we would need to shift c to index 2 for x to take its place, similarly if we want to insert x at index 1, we would need to shift both c and b to the right for x to be inserted. We notice that inserting at the first index of the array would result in the largest number of steps required for insertion. Namely, N + 1 steps, N shifting steps and 1 insertion step, where N is the size of the array. ### Arrays Delete Deleting is very similar to the [[Insert-Arrays]] operation. - Deleting the final element takes exactly one step. Assume we have a function delete, that removes the value stored at the passed index. ```js let arr = ['a','b','c']; arr.delete(2); the array would become ['a','b'] , however if we want to delete element b: let arr = ['a','b','c']; arr.delete(1); Thiss would result in removing the value b, then moving the value c from index 2 to index 1 to replace the location of the index that was occupied by b, this done to preserve the array as a data structure ofcontiguouss locations.Similarlyy if we want to delete the value at index 0, it would cost us a step for deletion, and two steps to shift b to index 0, and c to index 1. Consequently,in ann array of size N, deleting its zerothh element would cost us N steps.","title":"Arrays Insert"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/1-Why%20Data%20Structures%3F/#the-set-data-structure","text":"There are differentt types of sets, array-based sets in particular are identical to arrays with one additional constraints. The constraint being that it doesn't allow storing duplicates. This would have an implication on one of the operations, let's investigate the speed of each of the operations: 1. Read: it takes one step to read a value at a given index, just like an array. 2. Search: it takes the 1 step in the best case and N steps in the worst case to find an element in a set, just like arrays 3. Insertion: To insert an element into the set, we have to make sure that the element doesn't already exist in the set. Therefore, an insertion operation consists of search (to look for that element), and insertion steps to insert it. i.e in the best case, it would take N+1 steps to insert an element at the end of the array (N steps for search and 1 for insertion), and in the worst case, which is inserting an element at the beginning of the array it would require N steps of search. And N steps of shifting the elements to the right to free up space at the zeroth index and 1 step to insert the new value. i.e. a total of 2N+1 steps. 4. Deletion: it takes the same number of steps as arrays, one step for deletion and the number of elements to the right of the deleted element for shifting. If we're deleting the last element of the set, then we need only one step (to delete), if we are deleting the first element of an array of size N, then we need 1 step to delete the element and N-1 steps to shift the elements to the left, which is a total of N steps. To sum up: The cost of each operation in an array-based set: Operation Best Case Worst Case Read 1 step 1 step Search 1 step N steps Insert N+1 2N+1 steps Delete 1 step N steps","title":"The Set Data Structure"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/2-Why%20Algorithms%3F/","text":"Why Algorithms? \u00b6 An algorithm: a set of instructions to complete a specific task. We've seen the effect of choosing a data structure on the performance of our code, even similar data structures such as arrays and sets differ in behavior in an operation like insertion for example. Similarly, we will see that selecting a proper algorithm can have an effect on the efficiency of our code. Ordered Arrays as a Data Structure \u00b6 Ordered arrays are arrays where the order of elements is preserved. This fact would have an implication on two main operations carried out on sorted arrays, mainly insertion and search. Similar to what we've done for arrays, and sets. To study a data structure, we need to investigate how the four main operations are affected by choosing such a data structure. Ordered Arrays Search \u00b6 A superpower of ordered arrays is search. If we take linear search as the searching algorithm of choice (that's what we have in our arsenal until now), and search for an element in an ordered array, it would have a drastically different performance compared to searching through a regular array. If we are searching for the element 10 in the following regular array: 9 5 7 4 We would have to traverse every element in the array to ensure 10 doesn't exist in the array. That's N steps. To search for element 10 in the ordered array: |9|12|20|25| |---|---|---|---| It would take us 2 steps to determine that 10 doesn't exist in the array. Because in the first step we find 9, in the second we find 12, but since it's an ordered array, all the values to the right of 12 would be >= 12, but 10 < 12. Therefore, there is no way we would find 10 to the right of 12, so we stop our search costing us N/2 steps, that's half the number of steps required for searching a regular array. Let's create a function that looks for an element in a given array. const linearSearch(arr, searchValue){ for(let i=0; i< arr.length; i++){ if (arr[i] == searchValue){ return i;} } return null; } The previous function searches linearly through a regular array. It's not adapted to search through an ordered array. Adapting the function to search an ordered array implies that we need a condition that checks if the element we are comparing with is larger than searchValue , in that case we know that the element doesn't exist in the array, and we can exit the function. After modifying it, it looks something like this. function linearSearchV2(arr, searchValue){ for(let i=0; i<arr.length; i++){ if (arr[i]==searchValue){ return i;} else if (arr[i]>searchValue){ //exit, because we don't to look further return null; } return null; } } The previous function searches linearly through a regular array. It's not adapted to search through an ordered array. Adapting the function to search an ordered array implies that we need a condition that checks if the element we are comparing with is larger than searchValue , in that case we know that the element doesn't exist in the array, and we can exit the function. After modifying it, it looks something like this. \u200b\ufffcfunction linearSearchV2(arr, searchValue){ \u200b\ufffcfor(let i=0; i<arr.length; i++){ if (arr[i]==searchValue){ return i;} \u200b\ufffcelse if (arr[i]>searchValue){ //exit, because we don't to look further return null; } return null; } } In conclusion, searching through an ordered array can take less steps than searching through a regular array. But if the element we are looking for happens to be the last element of the array, then searching through an ordered array would take N steps, the same number of steps it takes to search through a regular array. The big advantage and super power if ordered arrays is that they allow us to use a much faster searching algorithm to search through them. Namely, the [[Binary Search]] algorithm. Binary Search \u00b6 If I asked you to guess a number that I have in my mind, it's between 1 and 100, and I will reply to your guesses with higher or lower. What's the first number you'd pick as your guess. Well, intuitively you'd pick 50, because it's right in the middle. If I told you higher, you've eliminated all the numbers less than 50, and if I tell you lower, you have eliminated half the numbers below 50. Let's say I tell you the number is lower than 50, now the problem becomes that you have to guess a number between 1 and 50, half the search space. So your second guess would be....25, again based on my answer, I am going to either eliminate all the numbers between 25 and 50. Or all the numbers between 1 and 25. This process can go on until the number we guess the number correctly. And it's much more efficient than random guesses. Because as we've seen, we are halving the search space at each step. If I asked you to guess a number that I have in my mind, it's between 1 and 100, and I will reply to your guesses with higher or lower. What's the first number you'd pick as your guess. Well, intuitively you'd pick 50, because it's right in the middle. If I told you higher, you've eliminated all the numbers less than 50, and if I tell you lower, you have eliminated half the numbers below 50. Let's say I tell you the number is lower than 50, now the problem becomes that you have to guess a number between 1 and 50, half the search space. So your second guess would be....25, again based on my answer, I am going to either eliminate all the numbers between 25 and 50. Or all the numbers between 1 and 25. This process can go on until the number we guess the number correctly. And it's much more efficient than random guesses. Because as we've seen, we are halving the search space at each step. This same logic applies to ordered arrays, we are looking for the value 10 for example in the ordered array: |5|10|15|20|25|30|35|40|45|50|55|60| |---|---|---|---|---|---|---|---|---|---|---|---| We start by comparing with the element in the middle of the array: |5|10|15|20|25|30|35|40|45|50|55|60| |---|---|---|---|---|---|---|---|---|---|---|---| ||||||?||||||| Is 10 higher or lower than 30, lower. Therefore, we eliminate the upper half of the array. And we don't need to look through it. |5|10|15|20|25|30|35|40|45|50|55|60| |---|---|---|---|---|---|---|---|---|---|---|---| ||||||x|x|x|x|x|x|x| we are left with the elements 5, 10, 15, 20, 25 to look through, so once again we start with comparing with the element in the middle of those elements: |5|10|15|20|25|30|35|40|45|50|55|60| |---|---|---|---|---|---|---|---|---|---|---|---| |||?|||x|x|x|x|x|x|x| Is 10 lower or higher than 15, it's lower. So, we eliminate the upper part of the subarray: |5|10|15|20|25|30|35|40|45|50|55|60| |---|---|---|---|---|---|---|---|---|---|---|---| |||x|x|x|x|x|x|x|x|x|x| We do the same step again, comparing with the element in the middle of the subarray 5, 10. Since we have an even number of elements, we can choose arbitrarily either the element to the right or to the left of the middle, in our example, the index in the middle of 1, 2 is 0.5, then we can either take 0 or 1 to start comparing. If we take 0: |5|10|15|20|25|30|35|40|45|50|55|60| |---|---|---|---|---|---|---|---|---|---|---|---| |?||x|x|x|x|x|x|x|x|x|x| Is 10 lower or higher than 5, higher. Then, we eliminate 5, and we are left with one element, 10. 10 = 10 ? Yes, which is the value we are looking for. It took us 4 steps to find the element we are looking for in an array of size 10, an operation that would take N steps if it was a linear search, and we were looking for an element at the end of the array. While, in binary search, it would also take us 4 steps to look for an element at the end of the array. Try it out! Binary search in code: const binarySearch(arr, searchValue){ let lower = 0; let upper = array.length - 1; while(lower!<= upper){ let mid = (lower+upper)/2; if (searchValue == arr[mid]){ //the element is found return mid; } else if (searchValue < mid){ //we need to look in the lower half upper = mid - 1; } else{ //we need to look in the upper half lower = mid +1; }} } Binary Search vs Linear search \u00b6 The real difference in performance between binary search and linear search is significant for large arrays, with an array containing 100 values, the maximum number of steps required to look for a value is: - Linear Search: 100 steps - Binary Search: 7 steps If we double the size of the array: - linear search would require additional steps equal to the number of added values to search for a value. - binary search would require one additional step to search for a required value. The difference between the performance of the two becomes more drastic as the size of the array increases. ![[Pasted image 20220803150647.png]] Reminder: Ordered Arrays aren't faster with respect to all operations, insertion for example in ordered array is slower than regular arrays. Therefore, by using an ordered array, we get a somewhat slower insertion, but a much faster search. Upgraded insertion: since we have binary search in our toolkit now, and since insertion into ordered arrays requires search before inserting an element, we can upgrade the search to binary search. However, insertion into regular arrays is still faster as they don't require search at all. Ordered Arrays Insertion \u00b6 Insertion: if we need to insert the value 6 into the following sorted array: 4 5 7 9 We can't directly append to the array as in the regular arrays case, because the ordering has to be preserved after the insertion, because it's an ordered array. Therefore, we would have to look for the proper location where the value 6 is going to be inserted. 4 5 7 9 X We first do the comparison: 6>4? 6 is larger than 4, so 6 has to be placed to the right of 4 4 5 7 9 X Then, we do the second comparison: 6>5? 6 is larger than 5, so 6 has to be placed to the right of 5 The third comparison would be with the 7: |4|5|7|9| |---|---|---|---| |||X|| 6 is less than 7, therefore, it would have to be inserted to the left of 7. To insert it to the left of 7, the 9 has to be shifted to the right (that's one step), and the 7 has to replace the 9 (that's two steps). Finally, after 5 steps, we reach the following state: 4 5 7 9 Now, the location where 6 should be inserted is unoccupied, and it can be inserted (an additional step). 4 5 6 7 9 So it took us 6 steps to insert in a 4 element sorted array. Which is N+2 steps. In this case, they were N/2 + 1steps of comparison, N/2 steps for shifting, and 1 insertion step. It turns out that the location where the element is going to be inserted doesn't affect the number of steps required, it just changes the distribution of the number of steps, between the steps required for shifting, and the steps required for comparison. For example, if we are inserting at the end of the sorted array, we would need N comparison steps, 0 shifts, and 1 insertion step, N+1 steps. If we are inserting at the beginning of sorted array, it would require us one comparison, N shifts, and 1 insertion. In conclusion, the insertion in sorted arrays takes N+1 steps in the best case, and N+2 steps in the worst case.","title":"2 Why Algorithms?"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/2-Why%20Algorithms%3F/#why-algorithms","text":"An algorithm: a set of instructions to complete a specific task. We've seen the effect of choosing a data structure on the performance of our code, even similar data structures such as arrays and sets differ in behavior in an operation like insertion for example. Similarly, we will see that selecting a proper algorithm can have an effect on the efficiency of our code.","title":"Why Algorithms?"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/2-Why%20Algorithms%3F/#ordered-arrays-as-a-data-structure","text":"Ordered arrays are arrays where the order of elements is preserved. This fact would have an implication on two main operations carried out on sorted arrays, mainly insertion and search. Similar to what we've done for arrays, and sets. To study a data structure, we need to investigate how the four main operations are affected by choosing such a data structure.","title":"Ordered Arrays as a Data Structure"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/2-Why%20Algorithms%3F/#ordered-arrays-search","text":"A superpower of ordered arrays is search. If we take linear search as the searching algorithm of choice (that's what we have in our arsenal until now), and search for an element in an ordered array, it would have a drastically different performance compared to searching through a regular array. If we are searching for the element 10 in the following regular array: 9 5 7 4 We would have to traverse every element in the array to ensure 10 doesn't exist in the array. That's N steps. To search for element 10 in the ordered array: |9|12|20|25| |---|---|---|---| It would take us 2 steps to determine that 10 doesn't exist in the array. Because in the first step we find 9, in the second we find 12, but since it's an ordered array, all the values to the right of 12 would be >= 12, but 10 < 12. Therefore, there is no way we would find 10 to the right of 12, so we stop our search costing us N/2 steps, that's half the number of steps required for searching a regular array. Let's create a function that looks for an element in a given array. const linearSearch(arr, searchValue){ for(let i=0; i< arr.length; i++){ if (arr[i] == searchValue){ return i;} } return null; } The previous function searches linearly through a regular array. It's not adapted to search through an ordered array. Adapting the function to search an ordered array implies that we need a condition that checks if the element we are comparing with is larger than searchValue , in that case we know that the element doesn't exist in the array, and we can exit the function. After modifying it, it looks something like this. function linearSearchV2(arr, searchValue){ for(let i=0; i<arr.length; i++){ if (arr[i]==searchValue){ return i;} else if (arr[i]>searchValue){ //exit, because we don't to look further return null; } return null; } } The previous function searches linearly through a regular array. It's not adapted to search through an ordered array. Adapting the function to search an ordered array implies that we need a condition that checks if the element we are comparing with is larger than searchValue , in that case we know that the element doesn't exist in the array, and we can exit the function. After modifying it, it looks something like this. \u200b\ufffcfunction linearSearchV2(arr, searchValue){ \u200b\ufffcfor(let i=0; i<arr.length; i++){ if (arr[i]==searchValue){ return i;} \u200b\ufffcelse if (arr[i]>searchValue){ //exit, because we don't to look further return null; } return null; } } In conclusion, searching through an ordered array can take less steps than searching through a regular array. But if the element we are looking for happens to be the last element of the array, then searching through an ordered array would take N steps, the same number of steps it takes to search through a regular array. The big advantage and super power if ordered arrays is that they allow us to use a much faster searching algorithm to search through them. Namely, the [[Binary Search]] algorithm.","title":"Ordered Arrays Search"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/2-Why%20Algorithms%3F/#binary-search","text":"If I asked you to guess a number that I have in my mind, it's between 1 and 100, and I will reply to your guesses with higher or lower. What's the first number you'd pick as your guess. Well, intuitively you'd pick 50, because it's right in the middle. If I told you higher, you've eliminated all the numbers less than 50, and if I tell you lower, you have eliminated half the numbers below 50. Let's say I tell you the number is lower than 50, now the problem becomes that you have to guess a number between 1 and 50, half the search space. So your second guess would be....25, again based on my answer, I am going to either eliminate all the numbers between 25 and 50. Or all the numbers between 1 and 25. This process can go on until the number we guess the number correctly. And it's much more efficient than random guesses. Because as we've seen, we are halving the search space at each step. If I asked you to guess a number that I have in my mind, it's between 1 and 100, and I will reply to your guesses with higher or lower. What's the first number you'd pick as your guess. Well, intuitively you'd pick 50, because it's right in the middle. If I told you higher, you've eliminated all the numbers less than 50, and if I tell you lower, you have eliminated half the numbers below 50. Let's say I tell you the number is lower than 50, now the problem becomes that you have to guess a number between 1 and 50, half the search space. So your second guess would be....25, again based on my answer, I am going to either eliminate all the numbers between 25 and 50. Or all the numbers between 1 and 25. This process can go on until the number we guess the number correctly. And it's much more efficient than random guesses. Because as we've seen, we are halving the search space at each step. This same logic applies to ordered arrays, we are looking for the value 10 for example in the ordered array: |5|10|15|20|25|30|35|40|45|50|55|60| |---|---|---|---|---|---|---|---|---|---|---|---| We start by comparing with the element in the middle of the array: |5|10|15|20|25|30|35|40|45|50|55|60| |---|---|---|---|---|---|---|---|---|---|---|---| ||||||?||||||| Is 10 higher or lower than 30, lower. Therefore, we eliminate the upper half of the array. And we don't need to look through it. |5|10|15|20|25|30|35|40|45|50|55|60| |---|---|---|---|---|---|---|---|---|---|---|---| ||||||x|x|x|x|x|x|x| we are left with the elements 5, 10, 15, 20, 25 to look through, so once again we start with comparing with the element in the middle of those elements: |5|10|15|20|25|30|35|40|45|50|55|60| |---|---|---|---|---|---|---|---|---|---|---|---| |||?|||x|x|x|x|x|x|x| Is 10 lower or higher than 15, it's lower. So, we eliminate the upper part of the subarray: |5|10|15|20|25|30|35|40|45|50|55|60| |---|---|---|---|---|---|---|---|---|---|---|---| |||x|x|x|x|x|x|x|x|x|x| We do the same step again, comparing with the element in the middle of the subarray 5, 10. Since we have an even number of elements, we can choose arbitrarily either the element to the right or to the left of the middle, in our example, the index in the middle of 1, 2 is 0.5, then we can either take 0 or 1 to start comparing. If we take 0: |5|10|15|20|25|30|35|40|45|50|55|60| |---|---|---|---|---|---|---|---|---|---|---|---| |?||x|x|x|x|x|x|x|x|x|x| Is 10 lower or higher than 5, higher. Then, we eliminate 5, and we are left with one element, 10. 10 = 10 ? Yes, which is the value we are looking for. It took us 4 steps to find the element we are looking for in an array of size 10, an operation that would take N steps if it was a linear search, and we were looking for an element at the end of the array. While, in binary search, it would also take us 4 steps to look for an element at the end of the array. Try it out! Binary search in code: const binarySearch(arr, searchValue){ let lower = 0; let upper = array.length - 1; while(lower!<= upper){ let mid = (lower+upper)/2; if (searchValue == arr[mid]){ //the element is found return mid; } else if (searchValue < mid){ //we need to look in the lower half upper = mid - 1; } else{ //we need to look in the upper half lower = mid +1; }} }","title":"Binary Search"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/2-Why%20Algorithms%3F/#binary-search-vs-linear-search","text":"The real difference in performance between binary search and linear search is significant for large arrays, with an array containing 100 values, the maximum number of steps required to look for a value is: - Linear Search: 100 steps - Binary Search: 7 steps If we double the size of the array: - linear search would require additional steps equal to the number of added values to search for a value. - binary search would require one additional step to search for a required value. The difference between the performance of the two becomes more drastic as the size of the array increases. ![[Pasted image 20220803150647.png]] Reminder: Ordered Arrays aren't faster with respect to all operations, insertion for example in ordered array is slower than regular arrays. Therefore, by using an ordered array, we get a somewhat slower insertion, but a much faster search. Upgraded insertion: since we have binary search in our toolkit now, and since insertion into ordered arrays requires search before inserting an element, we can upgrade the search to binary search. However, insertion into regular arrays is still faster as they don't require search at all.","title":"Binary Search vs Linear search"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/2-Why%20Algorithms%3F/#ordered-arrays-insertion","text":"Insertion: if we need to insert the value 6 into the following sorted array: 4 5 7 9 We can't directly append to the array as in the regular arrays case, because the ordering has to be preserved after the insertion, because it's an ordered array. Therefore, we would have to look for the proper location where the value 6 is going to be inserted. 4 5 7 9 X We first do the comparison: 6>4? 6 is larger than 4, so 6 has to be placed to the right of 4 4 5 7 9 X Then, we do the second comparison: 6>5? 6 is larger than 5, so 6 has to be placed to the right of 5 The third comparison would be with the 7: |4|5|7|9| |---|---|---|---| |||X|| 6 is less than 7, therefore, it would have to be inserted to the left of 7. To insert it to the left of 7, the 9 has to be shifted to the right (that's one step), and the 7 has to replace the 9 (that's two steps). Finally, after 5 steps, we reach the following state: 4 5 7 9 Now, the location where 6 should be inserted is unoccupied, and it can be inserted (an additional step). 4 5 6 7 9 So it took us 6 steps to insert in a 4 element sorted array. Which is N+2 steps. In this case, they were N/2 + 1steps of comparison, N/2 steps for shifting, and 1 insertion step. It turns out that the location where the element is going to be inserted doesn't affect the number of steps required, it just changes the distribution of the number of steps, between the steps required for shifting, and the steps required for comparison. For example, if we are inserting at the end of the sorted array, we would need N comparison steps, 0 shifts, and 1 insertion step, N+1 steps. If we are inserting at the beginning of sorted array, it would require us one comparison, N shifts, and 1 insertion. In conclusion, the insertion in sorted arrays takes N+1 steps in the best case, and N+2 steps in the worst case.","title":"Ordered Arrays Insertion"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/3-Big%20O%20Notation/","text":"Big O Notation \u00b6 We have concluded that what helps us determine the algorithm's performance is the number of steps an algorithm takes. However, it's impractical to say this algorithm takes 22 steps, and another takes 30 steps, etc. because the same algorithm can take a different number of steps based on the input it's operating on. Take linear search as an example, if we are searching through an array of 100 elements, then it's a 100-step algorithm, and if we are looking through an array of size 5, then we are looking at a 5-step algorithm. A much more efficient way of quantifying the efficiency of an algorithm is to represent the number of steps an algorithm takes as a function of the input size. Therefore, linear search is an algorithm that takes N steps given an array of N elements. Definition \u00b6 Big O is a formal mathematical notation used to quantify the efficiency of an algorithm, it's what the pros use instead of using mouthful statements. Big O notation attempts to answer the following question: Given N data elements, how many steps will the algorithm take? Search \u00b6 To elaborate: given N data elements, how many steps will the algorithm linear search take to search through the data, is equivalent to what is the big O of linear search? In which we answer it's O(N) Read \u00b6 Reading an element from the array is O(1), because this is the answer to the question: given an array of N elements, how many steps does it take to read an element. Well, one step. Therefore, reading is an operation that is dependent from the size of the array. O(1) are the fastest algorithms in existence, they are algorithms independent of the size of the data structure it's operating on. It takes these algorithms a constant number of steps to operate on a 100 element array, and the same time to operate on a 1000,000 element array. Definition Revisited \u00b6 Big O actually doesn't answer the question: Given N elements, how many operations does an algorithm take? It actually answers the questions, as the number of elements increase, how many operations does an algorithm take? Given this new definition, an algorithm that takes 3 steps to operate on the data, and one that takes 1 step, are not actually different, because they both take a constant number of steps as the number of elements they operate on changes. Therefore, these are algorithms of constant Big O, and we denote both with O(1). ![[Pasted image 20220803154603.png]] Asymptotic Consideration \u00b6 ![[Pasted image 20220803154711.png]] O(N) is considered a less efficient algorithm compared to O(1) no matter how many steps the O(1) algorithm actually takes. Because there will be a point where O(N) becomes less efficient than O(1). Binary Search in the Context of Big O \u00b6 Binary search is clearly not O(1), because the number of steps increase as the data increases. It is also not O(N) because the number of steps required as the data size increase is not the same as that increment. What is it then? It is between O(1) and O(N). It's O(log N), which is what we use to say that the algorithm takes one additional step as the data size doubles. ![[Pasted image 20220803160041.png]] Mathematically \u00b6 Logarithms are the inverse of exponents O(log N) means that for N data elements, the algorithm would take log2(N) steps. Or, given N elements, it would take us log(N) steps of dividing the elements in half until we reach a single element. ![[Pasted image 20220803160427.png]] The Big O depends on the scenario we are looking at, for example, linear search isn't always O(N). If the item we are looking for is at the end of the array it takes N steps, that's true. However, if the item is in the first cell of the array, then it'll take the search one step to find it, i.e. O(1). This implies that there are different Big Os for the algorithm based on the scenario. Linear search is O(N) in the worst case scenario, and it's O(1) in the best case scenario. Big O usually refers to the worst case scenario, because we want to be pessimistic in assessing the performance of our algorithm to prepare us for the worst case scenario.","title":"3 Big O Notation"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/3-Big%20O%20Notation/#big-o-notation","text":"We have concluded that what helps us determine the algorithm's performance is the number of steps an algorithm takes. However, it's impractical to say this algorithm takes 22 steps, and another takes 30 steps, etc. because the same algorithm can take a different number of steps based on the input it's operating on. Take linear search as an example, if we are searching through an array of 100 elements, then it's a 100-step algorithm, and if we are looking through an array of size 5, then we are looking at a 5-step algorithm. A much more efficient way of quantifying the efficiency of an algorithm is to represent the number of steps an algorithm takes as a function of the input size. Therefore, linear search is an algorithm that takes N steps given an array of N elements.","title":"Big O Notation"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/3-Big%20O%20Notation/#definition","text":"Big O is a formal mathematical notation used to quantify the efficiency of an algorithm, it's what the pros use instead of using mouthful statements. Big O notation attempts to answer the following question: Given N data elements, how many steps will the algorithm take?","title":"Definition"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/3-Big%20O%20Notation/#search","text":"To elaborate: given N data elements, how many steps will the algorithm linear search take to search through the data, is equivalent to what is the big O of linear search? In which we answer it's O(N)","title":"Search"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/3-Big%20O%20Notation/#read","text":"Reading an element from the array is O(1), because this is the answer to the question: given an array of N elements, how many steps does it take to read an element. Well, one step. Therefore, reading is an operation that is dependent from the size of the array. O(1) are the fastest algorithms in existence, they are algorithms independent of the size of the data structure it's operating on. It takes these algorithms a constant number of steps to operate on a 100 element array, and the same time to operate on a 1000,000 element array.","title":"Read"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/3-Big%20O%20Notation/#definition-revisited","text":"Big O actually doesn't answer the question: Given N elements, how many operations does an algorithm take? It actually answers the questions, as the number of elements increase, how many operations does an algorithm take? Given this new definition, an algorithm that takes 3 steps to operate on the data, and one that takes 1 step, are not actually different, because they both take a constant number of steps as the number of elements they operate on changes. Therefore, these are algorithms of constant Big O, and we denote both with O(1). ![[Pasted image 20220803154603.png]]","title":"Definition Revisited"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/3-Big%20O%20Notation/#asymptotic-consideration","text":"![[Pasted image 20220803154711.png]] O(N) is considered a less efficient algorithm compared to O(1) no matter how many steps the O(1) algorithm actually takes. Because there will be a point where O(N) becomes less efficient than O(1).","title":"Asymptotic Consideration"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/3-Big%20O%20Notation/#binary-search-in-the-context-of-big-o","text":"Binary search is clearly not O(1), because the number of steps increase as the data increases. It is also not O(N) because the number of steps required as the data size increase is not the same as that increment. What is it then? It is between O(1) and O(N). It's O(log N), which is what we use to say that the algorithm takes one additional step as the data size doubles. ![[Pasted image 20220803160041.png]]","title":"Binary Search in the Context of Big O"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/3-Big%20O%20Notation/#mathematically","text":"Logarithms are the inverse of exponents O(log N) means that for N data elements, the algorithm would take log2(N) steps. Or, given N elements, it would take us log(N) steps of dividing the elements in half until we reach a single element. ![[Pasted image 20220803160427.png]] The Big O depends on the scenario we are looking at, for example, linear search isn't always O(N). If the item we are looking for is at the end of the array it takes N steps, that's true. However, if the item is in the first cell of the array, then it'll take the search one step to find it, i.e. O(1). This implies that there are different Big Os for the algorithm based on the scenario. Linear search is O(N) in the worst case scenario, and it's O(1) in the best case scenario. Big O usually refers to the worst case scenario, because we want to be pessimistic in assessing the performance of our algorithm to prepare us for the worst case scenario.","title":"Mathematically"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/4-Bubble%20Sort/","text":"Bubble Sort created on: 2022-08-06 14:37 Sorting algorithms solve the following problem: Given an array of unsorted values, how can we sort them such that they end up in ascending order. Bubble sort algorithm: point to two consecutive values in an array starting from the first two and compare them. If the left value is larger than the one on the right, swap them. Otherwise, do nothing in this step. Move the two pointers to the right. Repeat the first three steps until we reach the end of the array. We move the two pointers to their initial position and repeat steps 1 through 4 (pass through) We keep doing pass throughs, until we reach a pass through in which no swaps take place. i.e. the array is already sorted. Example: 4 5 7 9 3 \u2191 \u2191 4 5 7 9 3 \u2191 \u2191 4 5 7 9 3 \u2191 \u2191 4 5 7 9 3 \u2191 \u2191 4 5 7 3 9 \u2191 \u2191 We are done with the first passthrough, where the 9 is properly placed. 4 5 7 3 9 \u2191 \u2191 4 5 7 3 9 \u2191 \u2191 4 5 7 3 9 \u2191 \u2191 4 5 3 7 9 \u2191 \u2191 4 5 3 7 9 \u2191 \u2191 We are done with the second pass through where the 7 is properly placed. 4 5 3 7 9 \u2191 \u2191 4 5 3 7 9 \u2191 \u2191 4 3 5 7 9 \u2191 \u2191 4 3 5 7 9 \u2191 \u2191 We are done with the third pass through where the 5 is properly placed. 4 3 5 7 9 \u2191 \u2191 3 4 5 7 9 \u2191 \u2191 The final pass through is done, where the 4 is properly placed, the array is now sorted. function bubbleSort(arr){ let lastIndex = arr.length - 1; while !(sorted){ let sorted = true for(let j=0; j<=lastIndex; j++){ if(arr[j]<arr[j+1]){ swap(arr[j], arr[j+1]) sorted = false } } lastIndex = lastIndex-1; } } The Bubble sort has two significant types of steps: - Comparisons: we iteratively keep comparing two consecutive numbers together to determine which one is greater. - Swaps: two numbers are swapped with one another in order to sort them. Let's see how many comparison steps, and how many swaps, are required for the bubble sort. Comparison steps: - first pass through: N-1 comparisons - second pass through: N-2 comparisons - third pass through: N-3 comparisons Swap steps: In the worst case (descending order), we would require a swap for each comparison, which is (N-1)+(N-2)+(N-3)+(N-4)+.......+1 swaps. Therefore, for an array of N values, we need: - (N-1)+(N-2)+(N-3)+(N-4)+.......+1 comparisons - (N-1)+(N-2)+(N-3)+(N-4)+.......+1 swaps - In both cases, this series is evaluated to N*(N-1)/2. Which is O(N 2 ). ![[Pasted image 20220806153131.png]] The bubble sort is a quadratic algorithm solving a [[quadratic problem]]. Quadratic Problem \u00b6 Let's create a function that determines whether an array has duplicate values or not. function checkDuplicates(arr){ for(let i=0; i<arr.length; i++){ for(let j=0; j< arr.length;j++){ if(i!=j && arr[i]==arr[j]){ return true; } } } return false; } It works, but is it sufficient? let's attempt to answer this with: 1) Big O For each iteration we are doing N comparisons, and we are doing N iterations, Therefore, the algorithm is O(N 2 ). 2) Tracking the number of steps function checkDuplicates(arr){ for(let i=0; i<arr.length; i++){ for(let j=0; j< arr.length;j++){ if(i!=j && arr[i]==arr[j]){ steps++; // to track the number of steps return true; } } } return false; } Since our algorithm is O(N 2 ), this should be a red flag, since O(N 2 ) is considered inefficient, and we should look for faster alternatives. Here is a linear solution to the same problem: function hasDuplicatesV2(arr){ let existinNumbers=[]; for(let i=0; i<=arr.length;i++){ if(existingNumbers[arr[i]] === 1){ return true; } else{ existingNumbers[arr[i]] =1; } } return false; } In the new version of hasDuplicates We are iterating over the array arr, and each step doing a comparison, therefore given array arr of N elements, we'll do N comparisons, therefore, the algorithm is O(N ), or a linear algorithm. However, it should be noted that we've used another array existingNmber to track the elements we've seen in arr , this means that although we have optimized for time complexity, we need more space for that. But for now we are focusing on time complexity, space complexity is a topic for another discussion. In conclusion, using Big O to analyze the efficiency of a given algorithm has enabled us to look for more efficient alternatives, and we've consequently found a linear algorithm to look for duplicates instead of the quadratic algorithm we used earlier.","title":"4 Bubble Sort"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/4-Bubble%20Sort/#quadratic-problem","text":"Let's create a function that determines whether an array has duplicate values or not. function checkDuplicates(arr){ for(let i=0; i<arr.length; i++){ for(let j=0; j< arr.length;j++){ if(i!=j && arr[i]==arr[j]){ return true; } } } return false; } It works, but is it sufficient? let's attempt to answer this with: 1) Big O For each iteration we are doing N comparisons, and we are doing N iterations, Therefore, the algorithm is O(N 2 ). 2) Tracking the number of steps function checkDuplicates(arr){ for(let i=0; i<arr.length; i++){ for(let j=0; j< arr.length;j++){ if(i!=j && arr[i]==arr[j]){ steps++; // to track the number of steps return true; } } } return false; } Since our algorithm is O(N 2 ), this should be a red flag, since O(N 2 ) is considered inefficient, and we should look for faster alternatives. Here is a linear solution to the same problem: function hasDuplicatesV2(arr){ let existinNumbers=[]; for(let i=0; i<=arr.length;i++){ if(existingNumbers[arr[i]] === 1){ return true; } else{ existingNumbers[arr[i]] =1; } } return false; } In the new version of hasDuplicates We are iterating over the array arr, and each step doing a comparison, therefore given array arr of N elements, we'll do N comparisons, therefore, the algorithm is O(N ), or a linear algorithm. However, it should be noted that we've used another array existingNmber to track the elements we've seen in arr , this means that although we have optimized for time complexity, we need more space for that. But for now we are focusing on time complexity, space complexity is a topic for another discussion. In conclusion, using Big O to analyze the efficiency of a given algorithm has enabled us to look for more efficient alternatives, and we've consequently found a linear algorithm to look for duplicates instead of the quadratic algorithm we used earlier.","title":"Quadratic Problem"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/5-Selection%20Sort/","text":"Selection sort operates as follows: 1) Check each cell of the array from left to right to find the minimum value 2) Once the minimum value is found we swap it with the value we were at when we started the pass through 3) A pass through consists of finding the minimum value, and swapping it with the value at the index we started with. We repeat the pass through until we reach a pass through where we start at the end of the array, which means that we have sorted the array. 4 5 7 9 3 \u2191 initial 4 5 7 9 3 \u2191 4 5 7 9 3 \u2191 4 5 7 9 3 \u2191 4 5 7 9 3 \u2191 swap, and advance initial pointer: 3 5 7 9 4 \u2191 initial 3 5 7 9 4 \u2191 3 5 7 9 4 \u2191 3 5 7 9 4 \u2191 3 5 7 9 4 \u2191 swap with initial, and advance initial: 3 4 7 9 5 \u2191 initial 3 4 7 9 5 \u2191 3 4 7 9 5 \u2191 swap with initial and advance initial: 3 4 5 9 7 \u2191 initial 3 4 5 9 7 \u2191 swap with initial and advance initial: 3 4 5 7 9 \u2191 initial Initial has reached the end of the array, therefore, the array is sorted and the algorithm terminates. Code implementation of selection sort: function selectionSort(arr){ for(let i=0; i<arr.length-1; i++){ let minIndex= i; let minVal=arr[i]; for(let j=i+1;j<arr.length; j++){ if (arr[j]<minVal){ minVal = arr[j]; minIndex = j; } } if (minIndex!=i){ let temp = arr[i]; arr[i] = arr[minIndex]; arr[minIndex] = temp; } console.log(arr) } return arr } Let's investigate the efficiency of selection sort, similar to bubble sort, it has two types of steps: comparisons and swaps: Comparisons: each iteration, we are comparing the value at initial with all the values in the array. That is, in the first iteration we do N-1 comparisons, then N-2 comparisons, then N-3, etc. Swaps: in each iteration we are either doing 1 swap, or no swaps at all. This depends on whether the minimum value is the one where initial is pointing at, or a new value that we found within the array. in the worst case scenario, we would do one swap each pass through, which is the case where the array is in reverse order. This means that in the worst case it costs us (N-1)+(N-2)+(N-3)+... comparisons, and 1+1+1+.... swaps, which is equivalent to N(N_1)/2 comparisons and N swaps. Which is O(N 2 ) if we go back to bubble sort, it takes N(N-1)/2 comparisons and N(N-1)/2 swaps to sort the array which is N(N-1). Selection sort on the other hand, takes N(N-1)/2 comparisons and N swaps, which is N(N-1)/2 + N ---> O(N 2 /2) compared to bubble sort, which is P(N 2 ). However, constants are irrelevant in Big O. Therefore, both algorithms have O(N 2 ). We drop constants in Big O because they become irrelevant and indicative of nothing for large inputs. Take for example O(100N) and O(N 2 ), for small input sizes O(100N) is less efficient, specifically for N<100, but once the input size reaches 100 and beyond, a O(100N) algorithm is much more efficient than one of O(N 2 ). ![[Pasted image 20220806174534.png]] Comparing O(100N) with O(N 2 ), is like comparing an apartment building with a skycrapper, it's like saying this is a 10 story building and this is a 100 floor skycrapper, the number of floors is irrelavent if we are comparing two different building categories.","title":"5 Selection Sort"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/6-Insertion%20Sort/","text":"Insertion Sort \u00b6 Although we've provided the rationale for considering the worst case scenario in our Big O calculation, because preparing for the worst case scenario would make sure we are in good shape. There are certain situations where considering all scenarios is an important skill. We've encountered two sorting algorithms until now, the bubble sort and selection sort, both requiring O(N 2 ) to run, selection sort being twice as fast. Insertion sort consists of the following steps: 1) In the first pass through, we remove the value at index 1 and put it in a temp variable. Then we start looking through the values to the left of the gap left by the value stored in the temp variable. 2) We begin the shifting phase, where for each value to the left of the gap, if that value is larger than the temp value we shift it to the right, which means the gap shifts to the left. We keep doing that until we encounter a value that is lower than the value in the temp that we are comparing to. Or if we reach the leftmost position of the array. 3) Once we reach that point, we insert the temp value in the gap. 4) We repeat steps 1 through 3, until the pass through begins at the final index of the array. Because by then the array would be sorted. 4 5 7 9 3 \u2191 initial 4 5 7 9 3 \u2191 initial 4 5 7 9 3 initial 4 5 7 9 3 \u2191 initial 4 5 7 9 3 \u2191 initial 4 5 7 9 3 initial 4 5 7 9 3 \u2191 initial 4 5 7 9 3 \u2191 initial 4 5 7 9 3 \u2191 initial 4 5 7 9 3 initial 4 5 7 9 3 \u2191 initial Begin the shifting phase: 4 5 7 9 initial 4 5 7 9 initial 4 5 7 9 initial 4 5 7 9 initial We have reached the leftmost of the array while shifting, therefore, we insert the value in temp in the current gap, which is at index 0. 3 4 5 7 9 initial Insertion Sort in code: function insertionSort(arr){ for(let i=1; i<arr.length; i++){ let temp = arr[i]; let j = i-1; while((j>=0)&&(arr[j]>temp)){ if(arr[j]>temp){ arr[j+1] = arr[j]; j--; } } arr[j+1] = temp; console.log(arr); }return arr;} To analyze the efficiency of the insertion sort, we look at 4 kinds of steps being done during the algorithm's execution: 1) removal (putting the value in temp) 2) Comparisons (comparing the values to the left of the index of the value to be inserted) to the value to be inserted. 3) Shifts (shifting the values larger than temp to the right) 4) Insertion (once we reach the location where the value is to be inserted, we insert it) Comparisons: comparisons take place each time we compare a value to the left of the gap to the temp value. The worst case scenario for comparisons, is to need to compare all the values to the left with temp value, this happens when the array is in reverse order. At the first pass through, we make one comparison which is the one element to the left of index 1, the second pass through we do two comparisons, etc., until the final pass through where the index is at the end of the array, then we do N-1 comparisons. Therefore, in the worst case we have 1+2+3+4+5+6+...N-1 comparisons. Which, as we have observed earlier, evaluates to N(N-1)/2, which is O(N 2 /2). Shifts occur each time the element is larger than the temp value, in the worst case, i.e. when the array is in reverse order, shifts happen as much as comparisons, which is N(N-1)/2 times. Shifts and comparisons are N(N-1)/2 + N(N-1)/2, which is N(N-1), and a total of O(N 2 ) With respect to removal and insertion, in the worst case, there is at max one removal and one insertion at each pass through. Since we have N-1 pass through, then we have (N-1) removals, and N-1 insertions. Finally, we have N 2 comparisons and shifts, and we have N-1 removals, and N-1 insertions, leaving us with a total of N 2 + 2N - 2 steps. Which is O(N 2 + N) after removing the constants. However, Big O has another rule. Which is: Big O only takes into account the highest order of N when we have multiple orders added together. Therefore, simplifying it down to the most dominant term, Insertion sort is also O(N 2 ). Insertion sort takes O(N 2 /2) in the [[Average Case]] Average Case \u00b6 created on: 2022-08-06 19:57 Although selection sort is faster than insertion sort in the worst case scenario, we need to inspect the average case because it's the case that is most frequent, i.e. more likely to occur. - Insertion sort in the best case (where the array is sorted) scenario would take one comparison each pass through and zero shifts. - Insertion sort in the average case is the average between its best case and worst case, we can say that on aggregate we compare and shift half of the data, therefore, we can say that insertion sort takes about N 2 steps on the average scenario. - We conclude that the performance of insertion sort varies significantly based on the scenario. - ![[Pasted image 20220806195436.png]] - selection sort on the other hand is O(N 2 /2) in all cases, because it has no mechanism to end a pass through early, it has to compare to all the values in a pass through. - ![[Pasted image 20220806195607.png]] - So, if we ask the question of which is better selection sort or insertion sort, it depends on the scenario we are encountering. If we suspect the arrays we are dealing with to be close to sorted, then insertion sort would be the better fit, while if we have a reason to believe that the arrays we are dealing with are going to be reversely sorted, then selection sort would be a better alternative, in the average case where we are not sure of the nature of the data we are operating on, there is no big difference between the two. Example: Finding the intersection between two arrays function findIntersection(arr1,arr2){ let intersection = []; for(let i =0; i<arr1.length; i++){ for(let j=0; j<arr2.length; j++){ if(arr1[i]===arr2[j]){ result.push(arr1[i]); } } } } This is O(N 2 ), in the worst case, the two arrays have no intersection, and therefore, for each element in arr1 we would have to go through every element in arr2 to look for an element in common. However, in the average case, there would be intersection between the two arrays. And therefore, the addition of a simple break after an element in common is found would reduce the number of steps required. function findIntersection(arr1, arr2){ let result = []; for(let i=0; i< arr1.length; i++){ for(let j=0; j< arr2.length; j++){ if(arr[i]===arr[j]){ result.push(arr[i]); break; } } } } Remember, while it's good to prepare for the worst case scenarios. Considering the average case is useful since it's the case that would occur most of the time.","title":"6 Insertion Sort"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/6-Insertion%20Sort/#insertion-sort","text":"Although we've provided the rationale for considering the worst case scenario in our Big O calculation, because preparing for the worst case scenario would make sure we are in good shape. There are certain situations where considering all scenarios is an important skill. We've encountered two sorting algorithms until now, the bubble sort and selection sort, both requiring O(N 2 ) to run, selection sort being twice as fast. Insertion sort consists of the following steps: 1) In the first pass through, we remove the value at index 1 and put it in a temp variable. Then we start looking through the values to the left of the gap left by the value stored in the temp variable. 2) We begin the shifting phase, where for each value to the left of the gap, if that value is larger than the temp value we shift it to the right, which means the gap shifts to the left. We keep doing that until we encounter a value that is lower than the value in the temp that we are comparing to. Or if we reach the leftmost position of the array. 3) Once we reach that point, we insert the temp value in the gap. 4) We repeat steps 1 through 3, until the pass through begins at the final index of the array. Because by then the array would be sorted. 4 5 7 9 3 \u2191 initial 4 5 7 9 3 \u2191 initial 4 5 7 9 3 initial 4 5 7 9 3 \u2191 initial 4 5 7 9 3 \u2191 initial 4 5 7 9 3 initial 4 5 7 9 3 \u2191 initial 4 5 7 9 3 \u2191 initial 4 5 7 9 3 \u2191 initial 4 5 7 9 3 initial 4 5 7 9 3 \u2191 initial Begin the shifting phase: 4 5 7 9 initial 4 5 7 9 initial 4 5 7 9 initial 4 5 7 9 initial We have reached the leftmost of the array while shifting, therefore, we insert the value in temp in the current gap, which is at index 0. 3 4 5 7 9 initial Insertion Sort in code: function insertionSort(arr){ for(let i=1; i<arr.length; i++){ let temp = arr[i]; let j = i-1; while((j>=0)&&(arr[j]>temp)){ if(arr[j]>temp){ arr[j+1] = arr[j]; j--; } } arr[j+1] = temp; console.log(arr); }return arr;} To analyze the efficiency of the insertion sort, we look at 4 kinds of steps being done during the algorithm's execution: 1) removal (putting the value in temp) 2) Comparisons (comparing the values to the left of the index of the value to be inserted) to the value to be inserted. 3) Shifts (shifting the values larger than temp to the right) 4) Insertion (once we reach the location where the value is to be inserted, we insert it) Comparisons: comparisons take place each time we compare a value to the left of the gap to the temp value. The worst case scenario for comparisons, is to need to compare all the values to the left with temp value, this happens when the array is in reverse order. At the first pass through, we make one comparison which is the one element to the left of index 1, the second pass through we do two comparisons, etc., until the final pass through where the index is at the end of the array, then we do N-1 comparisons. Therefore, in the worst case we have 1+2+3+4+5+6+...N-1 comparisons. Which, as we have observed earlier, evaluates to N(N-1)/2, which is O(N 2 /2). Shifts occur each time the element is larger than the temp value, in the worst case, i.e. when the array is in reverse order, shifts happen as much as comparisons, which is N(N-1)/2 times. Shifts and comparisons are N(N-1)/2 + N(N-1)/2, which is N(N-1), and a total of O(N 2 ) With respect to removal and insertion, in the worst case, there is at max one removal and one insertion at each pass through. Since we have N-1 pass through, then we have (N-1) removals, and N-1 insertions. Finally, we have N 2 comparisons and shifts, and we have N-1 removals, and N-1 insertions, leaving us with a total of N 2 + 2N - 2 steps. Which is O(N 2 + N) after removing the constants. However, Big O has another rule. Which is: Big O only takes into account the highest order of N when we have multiple orders added together. Therefore, simplifying it down to the most dominant term, Insertion sort is also O(N 2 ). Insertion sort takes O(N 2 /2) in the [[Average Case]]","title":"Insertion Sort"},{"location":"Data%20Structure%20%26%20Algorithms%20Project/6-Insertion%20Sort/#average-case","text":"created on: 2022-08-06 19:57 Although selection sort is faster than insertion sort in the worst case scenario, we need to inspect the average case because it's the case that is most frequent, i.e. more likely to occur. - Insertion sort in the best case (where the array is sorted) scenario would take one comparison each pass through and zero shifts. - Insertion sort in the average case is the average between its best case and worst case, we can say that on aggregate we compare and shift half of the data, therefore, we can say that insertion sort takes about N 2 steps on the average scenario. - We conclude that the performance of insertion sort varies significantly based on the scenario. - ![[Pasted image 20220806195436.png]] - selection sort on the other hand is O(N 2 /2) in all cases, because it has no mechanism to end a pass through early, it has to compare to all the values in a pass through. - ![[Pasted image 20220806195607.png]] - So, if we ask the question of which is better selection sort or insertion sort, it depends on the scenario we are encountering. If we suspect the arrays we are dealing with to be close to sorted, then insertion sort would be the better fit, while if we have a reason to believe that the arrays we are dealing with are going to be reversely sorted, then selection sort would be a better alternative, in the average case where we are not sure of the nature of the data we are operating on, there is no big difference between the two. Example: Finding the intersection between two arrays function findIntersection(arr1,arr2){ let intersection = []; for(let i =0; i<arr1.length; i++){ for(let j=0; j<arr2.length; j++){ if(arr1[i]===arr2[j]){ result.push(arr1[i]); } } } } This is O(N 2 ), in the worst case, the two arrays have no intersection, and therefore, for each element in arr1 we would have to go through every element in arr2 to look for an element in common. However, in the average case, there would be intersection between the two arrays. And therefore, the addition of a simple break after an element in common is found would reduce the number of steps required. function findIntersection(arr1, arr2){ let result = []; for(let i=0; i< arr1.length; i++){ for(let j=0; j< arr2.length; j++){ if(arr[i]===arr[j]){ result.push(arr[i]); break; } } } } Remember, while it's good to prepare for the worst case scenarios. Considering the average case is useful since it's the case that would occur most of the time.","title":"Average Case"},{"location":"Features/LaTeX%20Math%20Support/","text":"LaTeX Math Support \u00b6 index LaTeX math is supported using MathJax. Inline math looks like \\(f(x) = x^2\\) . The input for this is $f(x) = x^2$ . Use $...$ . For a block of math, use $$...$$ on separate lines $$ F(x) = \\int^a_b \\frac{1}{2}x^4 $$ gives \\[ F(x) = \\int^a_b \\frac{1}{2}x^4 \\]","title":"LaTeX Math Support"},{"location":"Features/LaTeX%20Math%20Support/#latex-math-support","text":"index LaTeX math is supported using MathJax. Inline math looks like \\(f(x) = x^2\\) . The input for this is $f(x) = x^2$ . Use $...$ . For a block of math, use $$...$$ on separate lines $$ F(x) = \\int^a_b \\frac{1}{2}x^4 $$ gives \\[ F(x) = \\int^a_b \\frac{1}{2}x^4 \\]","title":"LaTeX Math Support"},{"location":"Features/Mermaid%20Diagrams/","text":"Mermaid diagrams \u00b6 index Here's the example from MkDocs Material documentation : graph LR A[Start] --> B{Error?}; B -->|Yes| C[Hmm...]; C --> D[Debug]; D --> B; B ---->|No| E[Yay!];","title":"Mermaid diagrams"},{"location":"Features/Mermaid%20Diagrams/#mermaid-diagrams","text":"index Here's the example from MkDocs Material documentation : graph LR A[Start] --> B{Error?}; B -->|Yes| C[Hmm...]; C --> D[Debug]; D --> B; B ---->|No| E[Yay!];","title":"Mermaid diagrams"}]}